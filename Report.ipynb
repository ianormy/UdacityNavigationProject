{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The Deep Q-learning model used was created with PyTorch and has the following structure:\n",
    "\n",
    "![Model](Navigation_Model.png)\n",
    "\n",
    "The **input** to the model are the states, of which there are 37. These contain information about the agent's velocity, along with ray-based perception of objects around the agent's forward direction.\n",
    "\n",
    "There are 3 fully connected layers with **RELU** activations between them. Both the first and the second layers have 64 nodes in them.\n",
    "\n",
    "The **output** from the model are the actions, of which there are 4. These correspond to:\n",
    "- **`0`** - move forward.\n",
    "- **`1`** - move backward.\n",
    "- **`2`** - turn left.\n",
    "- **`3`** - turn right.\n",
    "\n",
    "The model class, **QNetwork**, is defined in the python file [model.py](model.py). Since it is a custom PyTorch model it derives from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I trained a Deep Q-Network (**DQN**) and then a Double-DQN (**DDQN**) model to solve the BananaCollector problem. I found that the **DDQN** performed much better than the **DQN**. It found a solution earlier and better than the **DQN**. This was to be expected, as the **DDQN** was proposed as an improvement to the original DQN model.\n",
    "\n",
    "The DDQN agent, **DDQNAgent** is defined in the python file [ddqn_agent.py](ddqn_agent.py). This uses a replay memory buffer, **ReplayBuffer**, which is defined in the python file [replay_buffer.py](replay_buffer.py)\n",
    "\n",
    "For more detailed explanations about **DQN** and **DDQN** please see my [Deep Q-Learning notebook](Deep_Q-Learning.ipynb) in this repository.\n",
    "\n",
    "### Training Model Weights\n",
    "I have included the training model weights in this repository. They are in this file: [ddqn_checkpoint.pth](ddqn_checkpoint.pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "I've created a separate notebook to show the results from the training I did using the two different models - **DQN** and **DDQN**. [Click here](Results.ipynb) to see it. I'm including the training summary below.\n",
    "![Results Summary](summary_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Pseudocode\n",
    "\n",
    "Below I show the pseudocode for the **DDQN** algorithm that I created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588); padding:10px 0;font-family:monospace;\">\n",
    "<ul>\n",
    "<li>Use an epsilon greedy strategy with a starting value of 1.0 and a minimum of 0.01 and a decay multiplier of 0.995</li>\n",
    "<li>limit the number of timesteps we can take for an episode, <font color=\"blue\">max_T</font>, to 1,000</li>\n",
    "<li>limit the number of training episodes, <font color=\"blue\">N</font>, to 2,000</li>\n",
    "</ul><br>\n",
    "Initialise the Q-learning agent's local model with state size (37) and action size (4)<br>\n",
    "Initialise the Q-learning agent's target model with the state size (37) and action size (4)<br>\n",
    "Initialise the ReplayBuffer<br>\n",
    "Initialise <font color=\"blue\">update_every</font> to 4<br>\n",
    "Initialise the list of scores for all training episodes<br>\n",
    "Initialise the list of the last 100 scores<br>\n",
    "<font color=\"blue\">epsilon</font> = 1.0<br>\n",
    "<b>for</b> <font color=\"blue\">i</font>=1, <font color=\"blue\">N</font> <b>do</b><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Reset the environment and obtain the current state <font color=\"blue\">S</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;<font color=\"blue\">score</font> = 0<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;<b>for</b> <font color=\"blue\">t</font>=1, <font color=\"blue\">max_T</font> <b>do</b><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get the action <font color=\"blue\">A</font> from the Q-learning agent for the given <font color=\"blue\">S</font> and <font color=\"blue\">epsilon</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;send the action <font color=\"blue\">A</font> to the environment and obtain the next state <font color=\"blue\">S'</font>, reward <font color=\"blue\">R</font> and <font color=\"blue\">done</font> (true if the episode has finished)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Save the (<font color=\"blue\">S</font>, <font color=\"blue\">A</font>, <font color=\"blue\">R</font>, <font color=\"blue\">S'</font>, <font color=\"blue\">done</font>) tuple in replay memory<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Increment the time step<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>if</b> this is an <font color=\"blue\">update_every</font> time step <b>then</b><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get max predicted Q values (for next states) from the local model<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute Q targets for current states<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Get expected Q values from the local model<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute loss<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Minimize the loss (using an Adam optimizer)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;back propogate the loss through the local model<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;update target model parameters <font color=\"blue\">θ_target</font> using <font color=\"blue\">τ</font>* local model parameters <font color=\"blue\">θ_local</font> + (1-<font color=\"blue\">τ</font>)* <font color=\"blue\">θ_target</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color=\"blue\">S</font> = <font color=\"blue\">S'</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color=\"blue\">score</font> = <font color=\"blue\">score</font> + <font color=\"blue\">R</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>if</b> <font color=\"blue\">done</font> <b>then</b><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;add the score to the list of the last 100 scores<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;add the score to the list of scores<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;<font color=\"blue\">epsilon</font> = maximum(0.01, <font color=\"blue\">epsilon</font> * 0.995)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if mean(last 100 scores) >= 13.0 then<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "There are still improvements that can be made! I would like to try **Dueling DQN** and also **Prioritized Experience Replay**. I am sure that both of these will produce improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "I would like to acknowledge that I started with original solutions that were provided by Udacity for the DQN. I have improved on this and used this as a basis for the DDQN.\n",
    "\n",
    "I also consulted the book [Grokking Deep Reinforcement Learning](https://www.manning.com/books/grokking-deep-reinforcement-learning) for algorithms, maths and explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
